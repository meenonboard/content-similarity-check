{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Relevant Comparation Example's Book\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import boto3, json\n",
    "import numpy as np\n",
    "\n",
    "AWS_ACCESS_KEY_ID=\"XXXXX\"\n",
    "AWS_SECRET_ACCESS_KEY=\"XXXXX\"\n",
    "AWS_SESSION_TOKEN=\"XXXXX\"\n",
    "\n",
    "bedrock_runtime = boto3.client(\n",
    "    'bedrock-runtime', \n",
    "    region_name=\"us-east-1\",\n",
    "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "    aws_session_token=AWS_SESSION_TOKEN,\n",
    "    )\n",
    "\n",
    "# Note :To setup openai on your local , https://pypi.org/project/openai/\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a document / Context\n",
    "document_context = '''\n",
    "Enter the context / document here.\n",
    "'''\n",
    "\n",
    "# Create Prompt\n",
    "user_prompt ='''\n",
    "Enter a prompt asking about the context / document here.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding service (COHERE) : Change normal texts into numerical data\n",
    "# Text can send with array format , has at least 1 element/array\n",
    " \n",
    "def get_relative_embeddings(bedrock_runtime , body_array:[]):\n",
    "    accept_message = \"application/json\"\n",
    "    content_type_message = \"application/json\"\n",
    "    model = \"cohere.embed-multilingual-v3\"\n",
    "    body_json = {\n",
    "        \"texts\": body_array,\n",
    "        \"input_type\":\"search_document\",\n",
    "    }\n",
    "    body_message = json.dumps(body_json, indent=2, ensure_ascii=False)\n",
    "    try:\n",
    "        response = bedrock_runtime.invoke_model(body=body_message,\n",
    "                                             contentType=content_type_message,\n",
    "                                             accept=accept_message,\n",
    "                                             modelId=model)\n",
    "        response_body = json.loads(response[\"body\"].read())\n",
    "        embeddings = response_body['embeddings']\n",
    "        return embeddings\n",
    "    except Exception as e:\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation function : Find how close of context between Text(a) and Text(b)\n",
    "\n",
    "def calculate_similarity(a, b):\n",
    "        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM function : Call Openai GPT 3.5 Turbo for generating text answer\n",
    "\n",
    "def get_openai_answer(document_context,prompt):\n",
    "    system_prompt = f'You are a helpful assistant.Please provide a concise summary of the following context: {document_context}'\n",
    "    max_n_token = 1024\n",
    "    completion_config = {\n",
    "        \"temperature\": 1,\n",
    "        \"max_tokens\": max_n_token,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(model='gpt-3.5-turbo',\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {'role': 'user', 'content': prompt},\n",
    "        ],\n",
    "        **completion_config)\n",
    "        openai_response_text = response.choices[0].message.content.strip()\n",
    "        return openai_response_text\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise e\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM function : Call Claude V2 for generating text answer\n",
    "\n",
    "def get_claude_v2_answer(document_context, prompt):\n",
    "    prompt = f'You are a helpful assistant.Please provide a concise summary ,based on given question, of the following context:{document_context}. Question:{prompt}'\n",
    "    accept_message = \"application/json\"\n",
    "    content_type_message = \"application/json\"\n",
    "    model_message = \"anthropic.claude-v2\"\n",
    "    body_message = json.dumps(\n",
    "        {\n",
    "            \"prompt\": \"\\n\\nHuman:\"+ prompt +\"\\n\\nAssistant:\",\n",
    "            \"max_tokens_to_sample\": 1024,\n",
    "        }\n",
    "    ).encode()\n",
    "\n",
    "    try:\n",
    "        bedrock_response = bedrock_runtime.invoke_model(body=body_message,\n",
    "                                            contentType=content_type_message,\n",
    "                                            accept=accept_message,\n",
    "                                            modelId=model_message)\n",
    "        claude_response_text = json.loads(bedrock_response[\"body\"].read())\n",
    "\n",
    "        return claude_response_text['completion']\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM function : Call Claude V3 for generating text answer\n",
    "\n",
    "def get_claude_v3_answer(document_context, prompt):\n",
    "    prompt = f'You are a helpful assistant.Please provide a concise summary ,based on given question, of the following context:{document_context}. Question:{prompt}'\n",
    "    accept_message = \"application/json\"\n",
    "    content_type_message = \"application/json\"\n",
    "    model_message = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "    messages_api_body = json.dumps(\n",
    "            {\n",
    "                \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                \"max_tokens\": 1000,\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\n",
    "                                \"type\": \"text\",\n",
    "                                \"text\": prompt\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        bedrock_response = bedrock_runtime.invoke_model(body=messages_api_body,\n",
    "                                            contentType=content_type_message,\n",
    "                                            accept=accept_message,\n",
    "                                            modelId=model_message)\n",
    "        claude_v3_response_body = json.loads(bedrock_response[\"body\"].read())\n",
    "        return claude_v3_response_body[\"content\"][0][\"text\"]\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start getting result from 3 LLMs\n",
    "openai_response_text = get_openai_answer(document_context=document_context,prompt=user_prompt)\n",
    "claude_v2_response_text = get_claude_v2_answer(document_context=document_context, prompt=user_prompt)\n",
    "claude_v3_response_text = get_claude_v3_answer(document_context=document_context, prompt=user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send Original Context to get embeddings\n",
    "text_original = [document_context]\n",
    "original_embeddings_result = get_relative_embeddings(bedrock_runtime=bedrock_runtime, body_array=text_original)\n",
    "\n",
    "# Send Openai Completion to get embeddings\n",
    "text_openai = [openai_response_text]\n",
    "openai_embeddings_result = get_relative_embeddings(bedrock_runtime=bedrock_runtime, body_array=text_openai)\n",
    "\n",
    "# Send ClaudeV2, V3 Completion to get embeddings\n",
    "text_pair_claude = [claude_v2_response_text,claude_v3_response_text]\n",
    "claude_embeddings_result = get_relative_embeddings(bedrock_runtime=bedrock_runtime, body_array=text_pair_claude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "             Name     Score\n",
      "0   GPT-3.5-Turbo  0.896029\n",
      "1        Claude-2  0.867688\n",
      "2  Claude-3-Haiku  0.896028\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# List1\n",
    "name = ['GPT-3.5-Turbo', 'Claude-2','Claude-3-Haiku',]\n",
    "# List2\n",
    "cosine_socre_openai = calculate_similarity(original_embeddings_result[0],openai_embeddings_result[0])\n",
    "cosine_score_claudeV2 = calculate_similarity(original_embeddings_result[0],claude_embeddings_result[0])\n",
    "cosine_score_claudeV3 = calculate_similarity(original_embeddings_result[0],claude_embeddings_result[1])\n",
    "score = [cosine_socre_openai, cosine_score_claudeV2,cosine_score_claudeV3,]\n",
    "\n",
    "# get the list of tuples from two lists.\n",
    "list_of_tuples = list(zip(name, score))\n",
    "\n",
    "# Converting lists of tuples into\n",
    "# pandas Dataframe.\n",
    "df = pd.DataFrame(list_of_tuples,\n",
    "\t\t\t\tcolumns=['Name', 'Score'])\n",
    "\n",
    "# Print score data.\n",
    "print('-' * 50)\n",
    "print(df)\n",
    "print('-' * 50)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Citation -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
